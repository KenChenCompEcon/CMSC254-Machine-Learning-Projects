{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bob_train = open('bobsue-data/bobsue.seq2seq.train.tsv').read().lower().split('\\n')[:-1]\n",
    "bob_test = open('bobsue-data/bobsue.seq2seq.test.tsv').read().lower().split('\\n')[:-1]\n",
    "bob_dev = open('bobsue-data/bobsue.seq2seq.dev.tsv').read().lower().split('\\n')[:-1]\n",
    "voc = list(set(open('bobsue-data/bobsue.voc.txt').read().lower().split('\\n')[:-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the training set\n",
    "x_train = []; y_train = []\n",
    "for p in bob_train:\n",
    "    pair = p.split('\\t')\n",
    "    x_train.append(pair[0]); y_train.append(pair[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of the vocabulary dictionary is: 1442\n"
     ]
    }
   ],
   "source": [
    "print(\"The size of the vocabulary dictionary is: {}\".format(len(voc)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try the randomized initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impelent the uniform distribution assignment\n",
    "torch.manual_seed(2)\n",
    "voc_w2n = {}\n",
    "num_rep = torch.FloatTensor(len(voc), 200).uniform_(-2, 2)\n",
    "for i in range(len(num_rep)):\n",
    "    voc_w2n[voc[i]] = num_rep[i]\n",
    "\n",
    "x_train_num = []; y_train_num = []\n",
    "for i in range(len(x_train)):\n",
    "    x_sen = [voc_w2n[word] for word in x_train[i].split()]\n",
    "    y_sen = [voc_w2n[word] for word in y_train[i].split()]\n",
    "    x_train_num.append(x_sen); y_train_num.append(y_sen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comment: Here I tried different range of uniform distributions to initialize the random word embedding. I thought when the wider the range, the more distinguishable between different words, and the computation can avoid some numerical issues computing very small derivatives. However, this does not seem to help much with the termical outcomes. And I finally ended up using the (-2, 2) range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLSTM(nn.Module):\n",
    "  \n",
    "    def __init__(self, n_in, n_out):\n",
    "        super(MyLSTM, self).__init__()\n",
    "        self.n_in = n_in\n",
    "        self.n_out = n_out\n",
    "        self.fc_enco = nn.Linear(n_in+n_out, n_out)\n",
    "        self.ic_enco = nn.Linear(n_in+n_out, n_out)\n",
    "        self.oc_enco = nn.Linear(n_in+n_out, n_out)\n",
    "        self.gc_enco = nn.Linear(n_in+n_out, n_out)\n",
    "        self.fc_deco = nn.Linear(n_in+n_out, n_out)\n",
    "        self.ic_deco = nn.Linear(n_in+n_out, n_out)\n",
    "        self.oc_deco = nn.Linear(n_in+n_out, n_out)\n",
    "        self.gc_deco = nn.Linear(n_in+n_out, n_out)\n",
    "\n",
    "    def forward(self, c_old, h_old, x, mode):\n",
    "        tensor = torch.cat([x, h_old])\n",
    "        if mode=='encode':\n",
    "            f = torch.sigmoid(self.fc_enco(tensor))\n",
    "            i = torch.sigmoid(self.ic_enco(tensor))\n",
    "            o = torch.sigmoid(self.oc_enco(tensor))\n",
    "            g = torch.tanh(self.gc_enco(tensor))\n",
    "            c = f*c_old + i*g\n",
    "            h = o*torch.tanh(c)\n",
    "        if mode=='decode':\n",
    "            f = torch.sigmoid(self.fc_deco(tensor))\n",
    "            i = torch.sigmoid(self.ic_deco(tensor))\n",
    "            o = torch.sigmoid(self.oc_deco(tensor))\n",
    "            g = torch.tanh(self.gc_deco(tensor))\n",
    "            c = f*c_old + i*g\n",
    "            h = o*torch.tanh(c)\n",
    "        return c, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The maximal length of all sentences is: 21\n"
     ]
    }
   ],
   "source": [
    "sen_length = [len(sen) for sen in y_train_num]\n",
    "print(\"The maximal length of all sentences is: {}\".format(max(sen_length)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I used the L2-Norm to compute the distance between words.\n",
    "def dist(h, y):\n",
    "    return torch.mean((h-y)**2)\n",
    "\n",
    "def compute_loss(h_lst, y_sen):\n",
    "    l_pred = len(h_lst); l_true = len(y_sen)\n",
    "    dim = len(y_sen[0])\n",
    "    if l_pred>=l_true:\n",
    "        # Here I multiplied at the end by (l_pred/l_true)**2, which is a penalty for failure to detect sentence end.\n",
    "        loss = sum([dist(h_lst[i], y_sen[i]) for i in range(l_true)])/l_true*((l_pred/l_true)**2)\n",
    "    else:\n",
    "        # When the predicted sentences are short than the ground true sentence, I made up the missing words by [0, ..., 0]\n",
    "        h_lst = h_lst + [2*torch.ones(y_sen[0].shape)]*(l_true-l_pred)\n",
    "        loss = sum([dist(h_lst[i], y_sen[i]) for i in range(l_true)])/l_true\n",
    "    return loss\n",
    "\n",
    "\n",
    "def find_closest(num_rep, h):\n",
    "    dists = ((num_rep-h)**2).sum(1)\n",
    "    return dists.min(0)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I chose to use the L2-norm loss as the softmax loss is way too computationally intensive for my computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0 | Loss 7.786\n",
      "-----------------------------------------------\n",
      "Epoch    1 | Loss 7.913\n",
      "-----------------------------------------------\n",
      "Epoch    2 | Loss 7.729\n",
      "-----------------------------------------------\n",
      "Epoch    3 | Loss 8.024\n",
      "-----------------------------------------------\n",
      "Epoch    4 | Loss 8.082\n",
      "-----------------------------------------------\n",
      "Epoch    5 | Loss 8.018\n",
      "-----------------------------------------------\n",
      "Epoch    6 | Loss 7.725\n",
      "-----------------------------------------------\n",
      "Epoch    7 | Loss 7.937\n",
      "-----------------------------------------------\n",
      "Epoch    8 | Loss 7.655\n",
      "-----------------------------------------------\n",
      "Epoch    9 | Loss 8.027\n",
      "-----------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Train the model with the training data set\n",
    "hparams = {\n",
    "    'learning_rate': 5,\n",
    "    'epochs': 10,\n",
    "}\n",
    "\n",
    "model_lst = []\n",
    "model = MyLSTM(200, 200)\n",
    "opt = optim.SGD(model.parameters(), lr=hparams['learning_rate'], momentum=0.8, weight_decay=0.6)\n",
    "epoch_losses = []\n",
    "torch.manual_seed(1234)\n",
    "np.random.seed(25400)\n",
    "for i in range(hparams['epochs']):\n",
    "    losses = []\n",
    "    for j in np.random.choice(len(x_train_num), 1000):\n",
    "        x_sen = x_train_num[j]\n",
    "        y_sen = y_train_num[j][1:]\n",
    "        c_old = torch.zeros(200)\n",
    "        h_old = torch.zeros(200)\n",
    "        for m in range(len(x_sen)):\n",
    "            x = x_sen[m]\n",
    "            c_old, h_old = model(c_old, h_old, x, 'encode')\n",
    "        x = voc_w2n['<s>']; word = '<s>'\n",
    "        h_lst = []; count = 0\n",
    "        while count<22 and word!='</s>':\n",
    "            c_old, h_old = model(c_old, h_old, x, 'decode')\n",
    "            h_lst.append(h_old)\n",
    "            idx = find_closest(num_rep, h_old)\n",
    "            x = num_rep[idx]; word = voc[idx]\n",
    "            count +=1\n",
    "        loss = compute_loss(h_lst, y_sen)\n",
    "        if j%200==0: opt.param_groups[0]['lr']=50 #make big jumps occasionally\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        if j%200==0: opt.param_groups[0]['lr']=hparams['learning_rate']\n",
    "        losses.append(loss)\n",
    "    loss_mean = sum(losses)/len(losses)\n",
    "    epoch_losses.append(loss_mean)\n",
    "    model_lst.append(deepcopy(model))\n",
    "    print('Epoch {:4} | Loss {:.3f}'.format(i, loss_mean))\n",
    "    print(\"-----------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I tried to implement more epochs, but it seems the RAM is running out of memory. But clearly, this does not work, the loss is not decreasing when averaging each epoch. (I inspected the first few initial losses associated with the first few sentence pairs, they are decreasing but stops going down very soon)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some commentes on tuning parameters:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sorry I did not take down all the results of attempts tuning up the parameters. But I would like to discuss about how the performance varied with different hyperparametric settings:\n",
    "* I tried various numbers of epochs. I guess it might be better to feed the model with a large enough training set size.\n",
    "* One of the problems I ran into is that the algorithm will quick converge to some sequence of words. Then I was told that by inflating the learning parameter we that add more momentum to the gradient descent algorithm. However, my final output does not work at all, so I guess we better not keep the learning rate constantly smalle or large. The optimal strategy might be makine big jumps occasionally, so that we neither get stuck at local minimum nor miss the global minimum all the time. I then tried this strategy, as shown in my codes, and the situations seems to have get better, but still needs more attempts reach success."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict the sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the testing set as random generated representations\n",
    "x_test = []; y_test = []\n",
    "for p in bob_test:\n",
    "    pair = p.split('\\t')\n",
    "    x_test.append(pair[0]); y_test.append(pair[1])\n",
    "\n",
    "x_test_num = []; y_test_num = []\n",
    "for i in range(len(x_test)):\n",
    "    x_sen = [voc_w2n[word] for word in x_test[i].split()]\n",
    "    y_sen = [voc_w2n[word] for word in y_test[i].split()]\n",
    "    x_test_num.append(x_sen); y_test_num.append(y_sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(word_lst, y_sen):\n",
    "    return np.mean([z[0]==z[1] for z in zip(word_lst, y_sen)])\n",
    "\n",
    "model_trained = model_lst[6]\n",
    "# Predict\n",
    "accuracy_lst = []; pred_sen = []\n",
    "for i in range(len(x_test)):\n",
    "    x_sen = x_test_num[i]\n",
    "    y_sen = y_test_num[i][1:]\n",
    "    c_old = torch.zeros(200)\n",
    "    h_old = torch.zeros(200)\n",
    "    for m in range(len(x_sen)):\n",
    "        x = x_sen[m]\n",
    "        c_old, h_old = model_trained(c_old, h_old, x, 'encode')\n",
    "    x = voc_w2n['<s>']; word = '<s>'\n",
    "    word_lst = [word]; h_lst = []; count = 0\n",
    "    while count<22 and word!='</s>':\n",
    "        c_old, h_old = model_trained(c_old, h_old, x, 'decode')\n",
    "        h_lst.append(h_old)\n",
    "        idx = find_closest(num_rep, h_old)\n",
    "        x = num_rep[idx]\n",
    "        word = voc[idx]; word_lst.append(word)\n",
    "        count +=1\n",
    "    accuracy = compute_accuracy(word_lst, y_test[i].split())\n",
    "    accuracy_lst.append(accuracy); pred_sen.append(\" \".join(word_lst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted sentences\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['<s> they line swimming hand line swimming seeing line swimming hand line swimming tape line swimming hand line swimming tape line swimming hand',\n",
       " '<s> they line swimming hand line swimming seeing line swimming hand line swimming tape line swimming hand line swimming tape line swimming hand',\n",
       " '<s> they line swimming hand line swimming seeing line swimming hand line swimming tape line swimming hand line swimming tape line swimming hand',\n",
       " '<s> they line swimming hand line swimming seeing line swimming hand line swimming tape line swimming hand line swimming tape line swimming hand',\n",
       " '<s> they line swimming hand line swimming seeing line swimming hand line swimming tape line swimming hand line swimming tape line swimming hand',\n",
       " '<s> they line swimming hand line swimming seeing line swimming hand line swimming tape line swimming hand line swimming tape line swimming hand',\n",
       " '<s> they line swimming hand line swimming seeing line swimming hand line swimming tape line swimming hand line swimming tape line swimming hand',\n",
       " '<s> they line swimming hand line swimming seeing line swimming hand line swimming tape line swimming hand line swimming tape line swimming hand',\n",
       " '<s> they line swimming hand line swimming seeing line swimming hand line swimming tape line swimming hand line swimming tape line swimming hand',\n",
       " '<s> they line swimming hand line swimming seeing line swimming hand line swimming tape line swimming hand line swimming tape line swimming hand']"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"The predicted sentences\")\n",
    "pred_sen[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The ground true sentences\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[\"<s> bob 's dad had his own basketball card when he was a professional . </s>\",\n",
       " \"<s> the picture didn 't look right . </s>\",\n",
       " '<s> she was tired and decided to sleep . </s>',\n",
       " '<s> she bought bob a brand new bike ! </s>',\n",
       " '<s> bob really loved his job ! </s>',\n",
       " '<s> his grades got better . </s>',\n",
       " '<s> he loved it so much ! </s>',\n",
       " '<s> she then asked if other people thought this , they said no . </s>',\n",
       " '<s> the same day his wife sue bought a new chair for the living room . </s>',\n",
       " '<s> he wanted to start dating . </s>']"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"The ground true sentences\")\n",
    "y_test[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average accuracy rate is: 0.09336658136611697\n"
     ]
    }
   ],
   "source": [
    "print(\"The average accuracy rate is: {}\".format(np.mean(accuracy_lst)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly the LSTM fails to predict any valid sentence, and the weak accuracy rate stems from the first $<s>$ tag."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try the precomputed initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "voc_dict = {}\n",
    "with open('bobsue-data/glove.6B.200d.txt', 'rb') as f:\n",
    "    for l in f:\n",
    "        line = l.decode().split()\n",
    "        word = line[0]\n",
    "        vect = np.array(line[1:]).astype(np.float)\n",
    "        voc_dict[word] = vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "voc_w2n_precom = {}\n",
    "num_rep_precom =[]\n",
    "for word in voc:\n",
    "    try:\n",
    "        v = voc_dict[word].tolist()\n",
    "        voc_w2n_precom[word] = torch.FloatTensor(v)\n",
    "        num_rep_precom.append(v)\n",
    "    except: # some of the words in our vocabulary are not documented in the glove dictionary\n",
    "        v = np.random.uniform(1,-1,200)\n",
    "        voc_w2n_precom[word] = torch.FloatTensor(v)\n",
    "        num_rep_precom.append(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rep_precom = torch.FloatTensor(num_rep_precom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the new representation of the training set when the pre-computed dictionary.\n",
    "x_train_num_precom = []; y_train_num_precom = []\n",
    "for i in range(len(x_train)):\n",
    "    x_sen = [voc_w2n_precom[word] for word in x_train[i].split()]\n",
    "    y_sen = [voc_w2n_precom[word] for word in y_train[i].split()]\n",
    "    x_train_num_precom.append(x_sen); y_train_num_precom.append(y_sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0 | Loss 192.772\n",
      "-----------------------------------------------\n",
      "Epoch    1 | Loss 223.981\n",
      "-----------------------------------------------\n",
      "Epoch    2 | Loss 133.779\n",
      "-----------------------------------------------\n",
      "Epoch    3 | Loss 219.012\n",
      "-----------------------------------------------\n",
      "Epoch    4 | Loss 221.925\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-2a4937bcd77d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m%\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mopt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparam_groups\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'lr'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[0mopt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m         \u001b[0mopt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m%\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mopt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparam_groups\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'lr'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'learning_rate'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    100\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m         \"\"\"\n\u001b[1;32m--> 102\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 90\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train the model with the training data set\n",
    "hparams = {\n",
    "    'learning_rate': 0.1,\n",
    "    'epochs': 30,\n",
    "}\n",
    "\n",
    "model_lst_precom = []\n",
    "model = MyLSTM(200, 200)\n",
    "opt = optim.SGD(model.parameters(), lr=hparams['learning_rate'], momentum=0.8, weight_decay=0.01)\n",
    "epoch_losses = []\n",
    "torch.manual_seed(1234)\n",
    "np.random.seed(1234)\n",
    "for i in range(hparams['epochs']):\n",
    "    losses = []\n",
    "    for j in np.random.choice(len(x_train_num), 1000):\n",
    "        x_sen = x_train_num_precom[j]\n",
    "        y_sen = y_train_num_precom[j][1:]\n",
    "        c_old = torch.zeros(200)\n",
    "        h_old = torch.zeros(200)\n",
    "        for m in range(len(x_sen)):\n",
    "            x = x_sen[m]\n",
    "            c_old, h_old = model(c_old, h_old, x, 'encode')\n",
    "        x = voc_w2n_precom['<s>']; word = '<s>'\n",
    "        h_lst = []; count = 0\n",
    "        while count<22 and word!='</s>':\n",
    "            c_old, h_old = model(c_old, h_old, x, 'decode')\n",
    "            h_lst.append(h_old)\n",
    "            idx = find_closest(num_rep_precom, h_old)\n",
    "            x = num_rep_precom[idx]; word = voc[idx]\n",
    "            count +=1\n",
    "        loss = compute_loss(h_lst, y_sen)*100\n",
    "        if j%100==0: opt.param_groups[0]['lr']=50\n",
    "        opt.zero_grad()\n",
    "        loss.backward(retain_graph=True)\n",
    "        opt.step()\n",
    "        if j%100==0: opt.param_groups[0]['lr']=hparams['learning_rate']\n",
    "        losses.append(loss)\n",
    "    loss_mean = sum(losses)/len(losses)\n",
    "    epoch_losses.append(loss_mean)\n",
    "    model_lst_precom.append(deepcopy(model))\n",
    "    print('Epoch {:4} | Loss {:.3f}'.format(i, loss_mean))\n",
    "    print(\"-----------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I interrupted the training as the loss is picking up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_num_precom = []; y_test_num_precom = []\n",
    "for i in range(len(x_test)):\n",
    "    x_sen = [voc_w2n_precom[word] for word in x_test[i].split()]\n",
    "    y_sen = [voc_w2n_precom[word] for word in y_test[i].split()]\n",
    "    x_test_num_precom.append(x_sen); y_test_num_precom.append(y_sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_trained_precom = model_lst_precom[2]\n",
    "# Predict\n",
    "losses = []; pred_sen = []\n",
    "for i in range(len(x_test)):\n",
    "    x_sen = x_test_num_precom[i]\n",
    "    y_sen = y_test_num_precom[i][1:]\n",
    "    c_old = torch.zeros(200)\n",
    "    h_old = torch.zeros(200)\n",
    "    for m in range(len(x_sen)):\n",
    "        x = x_sen[m]\n",
    "        c_old, h_old = model_trained_precom(c_old, h_old, x, 'encode')\n",
    "    x = voc_w2n['<s>']; word = '<s>'\n",
    "    word_lst = [word]; h_lst = []; count = 0\n",
    "    while count<22 and word!='</s>':\n",
    "        c_old, h_old = model_trained_precom(c_old, h_old, x, 'decode')\n",
    "        h_lst.append(h_old)\n",
    "        idx = find_closest(num_rep_precom, h_old)\n",
    "        x = num_rep_precom[idx]\n",
    "        word = voc[idx]; word_lst.append(word)\n",
    "        count +=1\n",
    "    loss = compute_loss(h_lst, y_sen)*100\n",
    "    losses.append(loss); pred_sen.append(\" \".join(word_lst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted sentences\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['<s> unfortunately addition addition addition addition addition addition addition addition addition addition addition addition addition addition addition addition addition addition addition addition addition',\n",
       " '<s> unfortunately addition addition addition addition addition addition addition addition addition addition addition addition addition addition addition addition addition addition addition addition addition',\n",
       " '<s> unfortunately addition addition addition addition addition addition addition addition addition addition addition addition addition addition addition addition addition addition addition addition addition',\n",
       " '<s> unfortunately addition addition addition addition addition addition addition addition addition addition addition addition addition addition addition addition addition addition addition addition addition',\n",
       " '<s> unfortunately addition addition addition addition addition addition addition addition addition addition addition addition addition addition addition addition addition addition addition addition addition',\n",
       " '<s> unfortunately addition addition addition addition addition addition addition addition addition addition addition addition addition addition addition addition addition addition addition addition addition',\n",
       " '<s> unfortunately addition addition addition addition addition addition addition addition addition addition addition addition addition addition addition addition addition addition addition addition addition',\n",
       " '<s> unfortunately addition addition addition addition addition addition addition addition addition addition addition addition addition addition addition addition addition addition addition addition addition',\n",
       " '<s> unfortunately addition addition addition addition addition addition addition addition addition addition addition addition addition addition addition addition addition addition addition addition addition',\n",
       " '<s> unfortunately addition addition addition addition addition addition addition addition addition addition addition addition addition addition addition addition addition addition addition addition addition']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"The predicted sentences\")\n",
    "pred_sen[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The ground true sentences\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[\"<s> bob 's dad had his own basketball card when he was a professional . </s>\",\n",
       " \"<s> the picture didn 't look right . </s>\",\n",
       " '<s> she was tired and decided to sleep . </s>',\n",
       " '<s> she bought bob a brand new bike ! </s>',\n",
       " '<s> bob really loved his job ! </s>',\n",
       " '<s> his grades got better . </s>',\n",
       " '<s> he loved it so much ! </s>',\n",
       " '<s> she then asked if other people thought this , they said no . </s>',\n",
       " '<s> the same day his wife sue bought a new chair for the living room . </s>',\n",
       " '<s> he wanted to start dating . </s>']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"The ground true sentences\")\n",
    "y_test[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, using the pre-computed word embedding fails again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some last comments:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All my model settings have failed no matter how I tune the learning rate and the number of epochs. And here are some other possible solutions I feel might work but did not try yet:\n",
    "* Change the archeture of MyLSTM class. Now I am chaining the encoder and decoder together, which forces the two sets of weights will be updated at the same learning rate when using the pyTorch optimizer. A better strategy might be to learn these parameters at different rates, as I checked the model weights and found the weight magnitudes at the encoder were significantly smaller than the decoder.\n",
    "* Another approach I have tried but did not have time to finish is to set the word embedding to be associated with a (1442,) vector, where the i'th word in the voc.txt takes the value of 1 at its associated vector. The benefits of doing so include that we can apply the softmax loss more easily, and each word outputs are easy to understand. The following is the model I experimented with this word representation, and with the softmax loss function. The training loss is declining but it is likely this approach will take forever for me to finish."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/KenChenCompEcon/hw08/blob/master/training_loss.png?raw=true\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
